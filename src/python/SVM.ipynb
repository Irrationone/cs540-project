{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "Blah"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "\n",
    "train = pd.DataFrame.from_csv('../../train.csv')\n",
    "test = pd.DataFrame.from_csv('../../test.csv')\n",
    "\n",
    "## Dimensions of train set\n",
    "ntrain,dtrain = train.shape\n",
    "\n",
    "## Dimensions of test set\n",
    "ntest, dtest = test.shape\n",
    "\n",
    "X = train.drop(['TARGET'], axis=1)\n",
    "\n",
    "## Add a column that tells us how many 0's we have\n",
    "X['n0'] = (X == 0).sum(axis=1)\n",
    "\n",
    "Xtest = test\n",
    "Xtest['n0'] = (Xtest == 0).sum(axis=1)\n",
    "\n",
    "## Standardize sets together\n",
    "def standardize_sets(X, Xtest):\n",
    "    Xtotal = X.append(Xtest)\n",
    "    Xtotal_scaled = preprocessing.scale(Xtotal)\n",
    "    X_scaled,Xtest_scaled = np.split(Xtotal_scaled, [ntrain])\n",
    "    return [X_scaled, Xtest_scaled]\n",
    "\n",
    "targets = np.array(train.TARGET)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/feature_selection/univariate_selection.py:113: UserWarning: Features [ 21  22  56  57  58  59  80  84  85 131 132 133 134 155 161 162 179 180\n",
      " 189 192 220 222 234 238 244 248 261 262 303 307 315 319 327 349] are constant.\n",
      "  UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.feature_selection import f_classif,chi2\n",
    "from sklearn.preprocessing import Binarizer, scale\n",
    "\n",
    "## Top 75th percentile\n",
    "p = 75\n",
    "y = targets\n",
    "\n",
    "X_bin = Binarizer().fit_transform(scale(X))\n",
    "selectChi2 = SelectPercentile(chi2, percentile=p).fit(X_bin, y)\n",
    "selectF_classif = SelectPercentile(f_classif, percentile=p).fit(X, y)\n",
    "\n",
    "chi2_selected = selectChi2.get_support()\n",
    "chi2_selected_features = [ f for i,f in enumerate(X.columns) if chi2_selected[i]]\n",
    "f_classif_selected = selectF_classif.get_support()\n",
    "f_classif_selected_features = [ f for i,f in enumerate(X.columns) if f_classif_selected[i]]\n",
    "selected = chi2_selected & f_classif_selected\n",
    "features = [ f for f,s in zip(X.columns, selected) if s]\n",
    "\n",
    "## Number of features selected\n",
    "print(len(features))\n",
    "\n",
    "X_selected = X[features]\n",
    "Xtest_selected = Xtest[features]\n",
    "\n",
    "X_selected,Xtest_selected = standardize_sets(X_selected, Xtest_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/preprocessing/data.py:167: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45612, 261)\n",
      "(30408, 261)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "sss = cross_validation.StratifiedShuffleSplit(targets, 1, test_size=0.4, random_state=4224)\n",
    "(train_index,valid_index) = list(sss)[0]\n",
    "\n",
    "X_train = X_selected[train_index,]\n",
    "X_valid = X_selected[valid_index,]\n",
    "y_train = targets[train_index]\n",
    "y_valid = targets[valid_index]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the thing (Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.68006226431640793)\n",
      "('Validation accuracy:', 0.68284661931070767)\n",
      "('Training AUC:', 0.71710942236112862)\n",
      "('Validation AUC:', 0.7153391132680198)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "lin_clf = svm.LinearSVC(dual=False, class_weight=\"balanced\")\n",
    "lin_clf.fit(X_train, y_train)\n",
    "\n",
    "y_hat = lin_clf.predict(X_valid)\n",
    "y_hat2 = lin_clf.predict(X_train)\n",
    "print(\"Training accuracy:\",lin_clf.score(X_train, y_train))\n",
    "print(\"Validation accuracy:\",lin_clf.score(X_valid, y_valid))\n",
    "print(\"Training AUC:\",roc_auc_score(y_train, y_hat2))\n",
    "print(\"Validation AUC:\",roc_auc_score(y_valid, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Validation AUC:', 0.7153391132680198)\n"
     ]
    }
   ],
   "source": [
    "preds = 1./(1.+np.exp(-y_hat))\n",
    "y_hatp = np.vstack((1-preds, preds)).T\n",
    "print(\"Validation AUC:\",roc_auc_score(y_valid, y_hatp[:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the thing (Polynomial kernel)\n",
    "This takes almost forever to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.75419297553275455)\n",
      "('Validation accuracy:', 0.74026571954748754)\n",
      "('Training AUC:', 0.72398845806217216)\n",
      "('Validation AUC:', 0.66330910388191489)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clfp = svm.SVC(kernel=\"poly\", degree=3, class_weight=\"balanced\")\n",
    "clfp.fit(X_train, y_train)\n",
    "\n",
    "y_hat = clfp.predict(X_valid)\n",
    "y_hat2 = clfp.predict(X_train)\n",
    "print(\"Training accuracy:\",clfp.score(X_train, y_train))\n",
    "print(\"Validation accuracy:\",clfp.score(X_valid, y_valid))\n",
    "print(\"Training AUC:\",roc_auc_score(y_train, y_hat2))\n",
    "print(\"Validation AUC:\",roc_auc_score(y_valid, y_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the thing (RBF kernel)\n",
    "This also takes almost forever to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training accuracy:', 0.70057221783741119)\n",
      "('Validation accuracy:', 0.69060773480662985)\n",
      "('Training AUC:', 0.75684607177485153)\n",
      "('Validation AUC:', 0.68205056331881908)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "clfp = svm.SVC(kernel=\"rbf\", class_weight=\"balanced\")\n",
    "clfp.fit(X_train, y_train)\n",
    "\n",
    "y_hat = clfp.predict(X_valid)\n",
    "y_hat2 = clfp.predict(X_train)\n",
    "print(\"Training accuracy:\",clfp.score(X_train, y_train))\n",
    "print(\"Validation accuracy:\",clfp.score(X_valid, y_valid))\n",
    "print(\"Training AUC:\",roc_auc_score(y_train, y_hat2))\n",
    "print(\"Validation AUC:\",roc_auc_score(y_valid, y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
